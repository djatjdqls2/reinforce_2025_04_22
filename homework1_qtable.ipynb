{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870269f5-76fd-4a5b-9d6d-077396e396e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "import gym_minigrid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3475c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, actions, agent_indicator=10):\n",
    "        self.actions = actions\n",
    "        self.agent_indicator = agent_indicator\n",
    "        self.alpha = 0.01\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.2\n",
    "        self.q_values = defaultdict(lambda: [0.0] * actions)\n",
    "\n",
    "    def _convert_state(self, obs):\n",
    "        y, x = np.where(obs == self.agent_indicator)\n",
    "        return (int(y[0]), int(x[0]))  # ← 정확한 좌표 기반 상태 반환\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        state = self._convert_state(state)\n",
    "        next_state = self._convert_state(next_state)\n",
    "        q_value = self.q_values[state][action]\n",
    "        max_next_q = max(self.q_values[next_state])\n",
    "        td_error = reward + self.gamma * max_next_q - q_value\n",
    "        self.q_values[state][action] = q_value + self.alpha * td_error\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            state = self._convert_state(state)\n",
    "            q_values = self.q_values[state]\n",
    "            return np.argmax(q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90706d6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import gen_wrapped_env, show_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e8c55c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 10\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(obs)\n\u001b[0;32m     11\u001b[0m ep_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mQLearning.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_state(state)\n\u001b[0;32m     27\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_values[state]\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(q_values)\n",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m, in \u001b[0;36mQLearning._convert_state\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[1;32m---> 11\u001b[0m     y, x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(obs \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_indicator)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(y[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(x[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from utils import gen_wrapped_env\n",
    "env = gen_wrapped_env('MiniGrid-Empty-6x6-v0')  # 6x6 환경\n",
    "\n",
    "agent = QLearning(actions=3, agent_indicator=10)\n",
    "\n",
    "episodes = 5000  # 충분히 학습\n",
    "for ep in range(episodes):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    action = agent.act(obs)\n",
    "    ep_rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        next_action = agent.act(next_obs)\n",
    "        agent.update(obs, action, reward, next_obs)\n",
    "        obs = next_obs\n",
    "        action = next_action\n",
    "        ep_rewards += reward\n",
    "\n",
    "    if (ep+1) % 500 == 0:\n",
    "        print(f\"Episode {ep+1}: Reward = {ep_rewards}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "{s:np.round(q, 5).tolist() for s, q in agent.q_values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ae590-aa2a-4187-8c03-09868e38f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# policy 추출\n",
    "policy = {s: np.argmax(q) for s, q in agent.q_values.items()}\n",
    "\n",
    "grid_size = 6\n",
    "policy_grid = np.full((grid_size, grid_size), -1)\n",
    "\n",
    "for (y, x), a in policy.items():\n",
    "    policy_grid[y, x] = a\n",
    "\n",
    "# MiniGrid-style 방향 매핑\n",
    "arrows = {0: '⟲', 1: '⟳', 2: '↑', -1: ' '}\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        action = policy_grid[i, j]\n",
    "        ax.text(j + 0.5, i + 0.5, arrows[action], ha='center', va='center', fontsize=18)\n",
    "\n",
    "ax.set_xticks(np.arange(grid_size + 1))\n",
    "ax.set_yticks(np.arange(grid_size + 1))\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.grid(True)\n",
    "ax.invert_yaxis()\n",
    "ax.set_title(\"Policy Heatmap (Start: (0,0) → Goal: (5,5))\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42787cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./logs\", exist_ok=True)\n",
    "\n",
    "pd.Series(rewards).to_csv('./logs/rewards_qlearning_empty.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da141489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sarsa_logs = pd.read_csv('./logs/rewards_sarsa.csv', index_col=False).iloc[:, 1]\n",
    "q_logs = pd.read_csv('./logs/rewards_qlearning_empty.csv', index_col=False).iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(q_logs.cumsum() / (pd.Series(np.arange(q_logs.shape[0])) + 1), label=\"Q-Learning\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Q-Learning: Cumulative Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"qlearning_empty.jpg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bef386-11b7-4fd0-a156-69f744a7f9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
